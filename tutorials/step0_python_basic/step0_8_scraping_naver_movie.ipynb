{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "웹 브라우저로 접근할 수 있는 페이지에 있는 정보 중, 각자가 원하는 정보를 선택하여 로컬 컴퓨터로 가져오는 과정을 스크래핑 (scraping)이라 하며, 크롤링 (crawling)이라고 불리고 있으나, 크롤링은 정확히는 스크래핑과 차이가 있는 의미입니다. \n",
    "\n",
    "[참고](http://stackoverflow.com/questions/4327392/what-is-the-difference-between-web-crawling-and-web-scraping)\n",
    "\n",
    "스크래핑을 하기 위하여 beautiful soup 4라는 HTML parser와 beautiful soup 4가 이용하는 lxml 이라는 XML parser를 이용합니다. 각자의 가상 환경에 해당 패키지가 없다면 아래의 명령어를 통하여 패키지를 설치할 수 있습니다. \n",
    "\n",
    "    pip install bs4\n",
    "    \n",
    "    pip install lxml\n",
    "\n",
    "이 외에도 다양한 스크래핑 도구들은 있습니다. \n",
    "\n",
    "또한 HTTP를 통하여 웹서버와 통신하기 위한 requests라는 패키지를 이용합니다. 이 역시 각자의 가상환경에 설치가 되어 있지 않다면 pip install requests를 하면 됩니다. anaconda 기본 패키지에는 위 세 도구 모두 설치되어 있습니다. \n",
    "\n",
    "    pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹페이지 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네이버 영화에서 각 영화들을 클릭해보면 url에 공통된 부분이 있습니다. \n",
    "\n",
    "    'http://movie.naver.com/movie/bi/mi/basic.nhn?code=134963' # 라라랜드\n",
    "    'http://movie.naver.com/movie/bi/mi/basic.nhn?code=126034' # 그래이트 워\n",
    "    'http://movie.naver.com/movie/bi/mi/basic.nhn?code=127382' # 조작된 도시\n",
    "    \n",
    "영화 아이디와 url의 공통된 부분을 합치면 각 영화에 해당하는 영화 url을 얻을 수 있습니다. 그렇기 때문에 url을 base와 id 부분으로 나눠서 만들며, 영화 아이디를 넣을 부분을 %s로 표시합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "url_basic_base = 'http://movie.naver.com/movie/bi/mi/basic.nhn?code=%s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라라랜드 영화 아이디는 movie_id = 134963 입니다. 라라랜드 영화와 관련된 메타 데이터를 수집해 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_id = 134963 # LaLa Land"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹페이지 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests는 네이버 영화 서버에 어떤 것을 요청할 수 있는 라이브러리입니다. requests의 requests.get(url)을 하면 해당 url의 서버로부터 웹페이지의 정보들을 얻어옵니다.\n",
    "requests.get(url)의 return은 텍스트 외에도 header와 같은 많은 정보들을 포함합니다. 이 중에서 우리가 필요한 것은 text (html source)이기 때문에 requests.get(url).text를 html로 저장합니다. html의 type은 str입니다.\n",
    "\n",
    "BeautifulSoup(html, 'lxml')은 스트링 형식의 html을 lxml이라는 XML parser를 이용하여 문서를 구조화합니다.\n",
    "웹페이지는 매우 긴 소스 코드로 이뤄져 있습니다. 브라우저 (크롬, 익스프롤러 등)는 이러한 복잡한 소스코드를 잘 구조화하여 화면에 보여주는 프로그램입니다. 매우 긴 코드지만 HTML은 구조화가 잘 되어 있습니다. 역으로 이 구조를 잘 파악하면 우리가 원하는 정보를 손쉽게 가져올 수 있는 것이죠. BeautifulSoup(html, 'lxml')을 한 번 실행함으로써 이미 HTML 문서는 다 구조화 되었습니다. 이제부터는 그 구조화된 문서로부터 우리가 원하는 정보를 가져올 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = url_basic_base % movie_id\n",
    "\n",
    "r = requests.get(url)\n",
    "html = r.text\n",
    "page = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests.get(url)의 결과물의 headers를 살펴보면, 해당 통신과 관련된 메타 정보들이 포함됨을 볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Content-Type': 'text/html;charset=UTF-8', 'Via': '1.1 varnish', 'Age': '0', 'Pragma': 'no-cache', 'P3P': 'CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\", CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\"', 'Content-Length': '37628', 'Date': 'Fri, 25 Aug 2017 15:48:16 GMT', 'X-Varnish-Cache': 'MISS', 'Server': 'Apache', 'Content-Encoding': 'gzip', 'Cache-Control': 'no-cache, no-store', 'Expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'X-Varnish': '3144830009', 'Accept-Ranges': 'bytes', 'Content-Language': 'ko-KR', 'connection': 'close', 'Vary': 'Accept-Encoding'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영화 제목 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영화의 영어 제목을 가져와 봅시다. \n",
    "\n",
    "![lalaland_1.png](lalaland_1.png)\n",
    "\n",
    "좌측 상단에 영화 이름이 있습니다. 원하는 정보를 드래그한 뒤, 크롬의 Inspect (한글은 요소 탐색)을 눌러보시면 해당 부분의 source code가 우측에 하이라이팅 되어 나타납니다. La La Land라는 영화 제목은 strong이라는 태그 안에 들어있으며, 그 태그의 class는 h_movie2입니다. HTML에서 태그라는 것은 \"<>\"으로 시작하여 \"</ >\"으로 끝나는 부분입니다. 링크의 경우에는 \"\\<a>\"로 시작하여 \"\\</a>\"로 끝납니다. \n",
    "\n",
    "\"\\<strong class=h_movie2\">\"는 \"\\<div class=mv_info>\"아래에 있다는 것도 볼 수 있습니다. \n",
    "\n",
    "    page.select('div[class=mv_info] strong[class=h_movie2]')\n",
    "\n",
    "위 코드는 mv_info라는 클래스 이름을 갖는 div 아래에 속한, class 이름이 h_movie2인 strong이라는 것을 찾아서 가져온다는 의미입니다. \n",
    "\n",
    "![lalaland_2.png](lalaland_2.png)\n",
    "\n",
    "select의 결과는 하나가 아닐 수 있기 때문에 return type은 list입니다. 실제 우리의 데이터에서도 select에 해당하는 부분이 2개가 있었네요. 그 중 첫번째 부분만을 이용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "title = page.select('div[class=mv_info] strong[class=h_movie2]')\n",
    "print(type(title))\n",
    "print(len(title), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<strong class=\"h_movie2\" title=\"La La Land\t\t\t\t\t\t\t\t\t\t, \t\t\t\t\t2016\">La La Land\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t, \r\n",
      "\t\t\t\t\t2016</strong>, <strong class=\"h_movie2\" title=\"La La Land, 2016\">La La Land, 2016</strong>] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(title, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong class=\"h_movie2\" title=\"La La Land\t\t\t\t\t\t\t\t\t\t, \t\t\t\t\t2016\">La La Land\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t, \r\n",
      "\t\t\t\t\t2016</strong> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(title[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong class=\"h_movie2\" title=\"La La Land, 2016\">La La Land, 2016</strong> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(title[1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup.select()의 return type은 bs4에서 만들어둔 클래스입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "print(type(title[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bs4.element.Tag 안에는 HTML tag의 attribute 종류나 값과 같은 태그 정보를 가져오거나 텍스트 부분을 가져올 수 있는 기능이 있습니다. 영화 제목 La La Land, 2016은 텍스트 부분에 있으니 이를 가져오겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La La Land\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t, \\r\\n\\t\\t\\t\\t\\t2016'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\r\\n 과 같은 줄바꿈 기호나 \\t과 같은 탭, 띄어쓰기 때문에 텍스트가 깔끔해 보이지 않습니다. 이를 제거하여 깔끔한 영화 제목을 가져옵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La La Land, 2016'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title[0].text.replace(\"\\t\", '').replace('\\r', '').replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비슷하게 한국어 영화 제목도 가져와봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'라라랜드'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = page.select('div[class=mv_info] h3[class=h_movie] a')\n",
    "title[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네이버 영화 페이지는 모든 영화에 대하여 각 영화에 해당하는 페이지의 내용만 바뀌며, 그 형식은 일정합니다. 즉 템플릿이 존재하는 웹페이지에서 스크래핑을 할 때는 해당 웹페이지들의 구조를 파악하면 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try - except를 통한 에러 방지\n",
    "\n",
    "for loop을 돌면서 여러 영화의 메타 정보를 가져오겠습니다. 그런데 중간에 Exception이 날 수 있습니다. 인터넷이 끊길 수도 있고, 형식이 잘 맞지 않는 HTML이 있을 수도 있습니다. 이 때 한 번 오류가 나면 프로그램이 멈출텐데, 오류가 나는 영화는 건너띄고 다음 영화의 정보를 얻어오고 싶다면 try - except 구문을 이용하면 됩니다\n",
    "\n",
    "아래의 코드는 i가 3일때 3앞에 a라는 문자를 붙여서 출력하는 코드입니다. 그리고 그 아래 코드는 i가 3일때 a를 붙인 뒤, j를 다시 integer로 casting하는 코드입니다. 그렇다면 i=3 일 때에는 a3을 인티저로 캐스팅 하지 못하여 오류가 납니다. 그리고 그 다음 i=4 일때는 실행이 되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "a3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):    \n",
    "    \n",
    "    s = str(i) if i != 3 else 'a%d' % i\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s = 0, j = 0\n",
      "s = 1, j = 1\n",
      "s = 2, j = 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'a3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-447d792fc391>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'a%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m's = %s, j = %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'a3'"
     ]
    }
   ],
   "source": [
    "for i in range(5):    \n",
    "    \n",
    "    s = str(i) if i != 3 else 'a%d' % i    \n",
    "    j = int(s)\n",
    "    \n",
    "    print('s = %s, j = %d' % (s, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오류가 날 수 있는 부분을 try - except로 감싸면 오류가 난 경우 print(e)에 의하여 오류 형태를 출력해주고 다음 for loop (i=4) 일때로 넘어가게 됩니다. Exception을 e라는 이름으로 받은 뒤 except 안에서 출력하면 해당 예외가 무엇이었는지 알 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s = 0, j = 0\n",
      "s = 1, j = 1\n",
      "s = 2, j = 2\n",
      "invalid literal for int() with base 10: 'a3'\n",
      "s = 4, j = 4\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "\n",
    "for i in range(5):    \n",
    "    try:\n",
    "        s = str(i) if i != 3 else 'a%d' % i    \n",
    "        j = int(s)\n",
    "\n",
    "        print('s = %s, j = %d' % (s, j))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여러 영화에 대하여 영화마다 제목 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기능별로 함수화를 하면 코드가 가독성이 좋아집니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    try:\n",
    "        r = requests.get(url).text\n",
    "        return BeautifulSoup(r, 'lxml')\n",
    "    except Exception as e:\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        traceback_details = {\n",
    "                     'filename': exc_traceback.tb_frame.f_code.co_filename,\n",
    "                     'lineno'  : exc_traceback.tb_lineno,\n",
    "                     'name'    : exc_traceback.tb_frame.f_code.co_name,\n",
    "                     'type'    : exc_type.__name__,\n",
    "                     'message' : str(e)\n",
    "                    }\n",
    "        pprint(traceback_details)\n",
    "        return ''\n",
    "\n",
    "def _parse_title(page):\n",
    "    try: \n",
    "        return page.select('div[class=mv_info] h3[class=h_movie] a')[0].text.strip()\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "range는 range(b, e)에 대하여 b부터 e까지의 숫자를 1씩 증가시키며 yield (return과 비슷) 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134960: 그놈이다\n",
      "134961: 어 라 말라\n",
      "134962: 아이\n",
      "134963: 라라랜드\n",
      "134964: 콜 포 헬프\n"
     ]
    }
   ],
   "source": [
    "for movie_id in range(134960, 134965):\n",
    "    url = url_basic_base % movie_id\n",
    "    page = get_soup(url)\n",
    "    title = _parse_title(page)\n",
    "    print('%d: %s' % (movie_id, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "range(b, e, s)를 하면 b 부터 e 까지 s 간격으로 출력됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1, 6, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packing\n",
    "\n",
    "코드를 짤 때, 기능별로 함수를 나눠서 적어두면 좋습니다. parse_basic_page라는 함수를 보면, 제목을 가져오는 부분, 장르를 가져오는 부분 등을 나눠서 적어두었습니다. 가독성을 높여주며, 코드에 오류가 있을 때 수정하기가 용이해집니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic pages\n",
    "def get_basic_page(movie_id):\n",
    "    url = url_basic_base % movie_id\n",
    "    return get_soup(url)\n",
    "\n",
    "def parse_basic_page(page):\n",
    "    movie = {}\n",
    "\n",
    "    score = _parse_main_score(page)\n",
    "    movie['expert_score'] = score[0]\n",
    "    movie['netizen_score'] = score[1]\n",
    "\n",
    "    movie['title'] = _parse_title(page)\n",
    "    movie['e_title'] = _parse_e_title(page)\n",
    "\n",
    "    try:\n",
    "        basic_inf = page.select('dl[class=info_spec]')[0]\n",
    "        movie['genres'] = _parse_genres(page)\n",
    "        movie['countries'] = _parse_countries(page)\n",
    "        movie['running_time'] = _parse_running_time(page)\n",
    "        movie['open_dates'] = _parse_open_date(page)\n",
    "        movie['grade'] = _parse_grade(page)\n",
    "        return movie\n",
    "    except Exception as e:\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        traceback_details = {\n",
    "                     'filename': exc_traceback.tb_frame.f_code.co_filename,\n",
    "                     'lineno'  : exc_traceback.tb_lineno,\n",
    "                     'name'    : exc_traceback.tb_frame.f_code.co_name,\n",
    "                     'type'    : exc_type.__name__,\n",
    "                     'message' : str(e)\n",
    "                    }\n",
    "        return movie\n",
    "    \n",
    "def _parse_title(page):\n",
    "    try: \n",
    "        return page.select('div[class=mv_info] h3[class=h_movie] a')[0].text.strip()\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def _parse_e_title(page):\n",
    "    try:\n",
    "        return page.select('div[class=mv_info] strong[class=h_movie2]')[0].text.replace('\\r', '').replace('\\t', '').replace('\\n', '').strip()\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def _parse_genres(page):\n",
    "    genres = page.select('a[href^=/movie/sdb/browsing/bmovie.nhn?genre=]')\n",
    "    return list({genre.text for genre in genres})\n",
    "    \n",
    "def _parse_countries(page):\n",
    "    countries = page.select('a[href^=/movie/sdb/browsing/bmovie.nhn?nation=]')\n",
    "    return list({country.text for country in countries})\n",
    "    \n",
    "def _parse_running_time(page):\n",
    "    running_time = 0\n",
    "    try:\n",
    "        running_time = re.search(r\"\\d+분\", page.text).group()[:-1]\n",
    "    except:\n",
    "        running_time = 0\n",
    "    return running_time\n",
    "\n",
    "def _parse_open_date(page):\n",
    "    return list({d for d in re.findall(r\"\\d+\\.\\d+\\.\\d+ 재*개봉\", page.text)})\n",
    "\n",
    "def _parse_grade(page):\n",
    "    try:\n",
    "        return page.select('a[href^=/movie/sdb/browsing/bmovie.nhn?grade]')[0].text\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def _parse_main_score(page):\n",
    "    try:\n",
    "        main_score = page.select('div[class=main_score]')[0]\n",
    "        expert_score = main_score.select('div[class=spc_score_area] div[class=star_score]')[0].text.replace('\\n','')\n",
    "        netizen_score = main_score.select('div[class=score] div[class=star_score] span[class=st_off]')[0].text.replace('관람객 평점 ', '').replace('점', '')\n",
    "        return expert_score, netizen_score\n",
    "    except Exception as e:\n",
    "        # print('error from _parse_main_score', e)\n",
    "        return -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countries': ['미국'],\n",
       " 'e_title': 'La La Land, 2016',\n",
       " 'expert_score': '8.34',\n",
       " 'genres': ['뮤지컬', '드라마', '멜로/로맨스'],\n",
       " 'grade': '12세 관람가',\n",
       " 'netizen_score': '8.87',\n",
       " 'open_dates': [],\n",
       " 'running_time': '127',\n",
       " 'title': '라라랜드'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_id = 134963\n",
    "url = url_basic_base % movie_id\n",
    "page = get_soup(url)\n",
    "parse_basic_page(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 이런 작업들은 네이버 영화 서버의 입장에서는 디도스 공격과 다르지 않습니다. 알지 못하는 컴퓨터에서 비정상적으로 많은 requests를 요청하는 것이기 때문입니다. 가끔씩 어떤 서버들은 이러한 스크래핑 작업을 공격으로 오인하여 아이피를 차단하기도 합니다. 이를 막기 위해서는 적당히 쉬엄쉬엄 크롤링을 하는게 좋습니다. for loop을 돌면서 프로그램을 원하는 만큼 쉴 수 있습니다. 아래 코드는 for loop을 돌며 한 번 숫자를 출력한 뒤, 1.0초를 쉬어주는 것입니다. 영화 제목을 긁을 때에도 영화마다 어느 정도 시간을 주며 쉬어주는 것 (sleep)이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "134960: 그놈이다\n",
      "134961: 어 라 말라\n",
      "134962: 아이\n",
      "134963: 라라랜드\n",
      "134964: 콜 포 헬프\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    time.sleep(1.0)\n",
    "    \n",
    "for movie_id in range(134960, 134965):\n",
    "    url = url_basic_base % movie_id\n",
    "    page = get_soup(url)\n",
    "    title = _parse_title(page)\n",
    "    print('%d: %s' % (movie_id, title))\n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User agent 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 네이버 영화 서버의 입장에서, 위의 코든느 자신의 정보를 제공하지 않은 체, 무명으로 접근하는 프로그램으로 인식할 수 있습니다. 어떤 서버들은 (예, IMDB) 이러한 요청에 대해서는 답변을 주지 않습니다. 이를 해결하기 위하여 requests를 보낼 때, 자신이 누구인지에 대한 정보를 넣어주면 좋습니다. user-agent를 header에 넣어서 requests를 보낼 수 있습니다. \n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "위와 같이 requests.get을 할 때, headers에 자신에 대한 정보를 적어주면 좋습니다. 어떤 user-agent를 적어야 하는지는 구글에 python requests user agent라는 제목으로 검색을 해보시면 많은 종류가 나올 겁니다. \n",
    "\n",
    "아래는 참고한 stackoverflow 주소입니다. fake_useragent라는 라이브러리도 있다고 합니다.\n",
    "\n",
    "http://stackoverflow.com/questions/27652543/how-to-use-python-requests-to-fake-a-browser-visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일 다운로드\n",
    "\n",
    "파이썬에서도 음악/사진 파일들을 다운로드 받을 수 있습니다. 다운로드라는 것도 서버와 내 컴퓨터 간의 통로를 열어두고, 전송이 되는 byte 정보들을 모아서 다시 음악, 사진 포멧으로 읽는 것입니다. 물론 스트리밍 서비스들은 temporal하게 내 컴퓨터에 데이터가 쌓이지 않게 막을 수도 있습니다. 그런 종류가 아니라, \\<a>라는 태그로 링크가 걸려있는 이미지 파일들을 다운로드 해보겠습니다. \n",
    "\n",
    "아래 코드는 urllib.request.urlopen을 통하여 서버와 내 컴퓨터 간의 통신 통로를 열어둡니다. while loop 안에서 열려진 통로에서 100000000 byte만큼의 정보를 가져와 buffer에 넣습니다. 그리고 이 정보를 미리 열어둔 downloaded_file이라는 파일에 적습니다. 주고 받은 정보가 텍스트가 아니라 바이트이기 때문에 'wb'로 파일을 열어둡니다. 다운로드가 모두 끝나면 열어둔 통신 서버를 닫고\n",
    "    \n",
    "    opened.close()\n",
    "    \n",
    "열어둔 파일도 닫습니다. \n",
    "\n",
    "    downloaded_file.close()\n",
    "    \n",
    "다운로드가 성공적으로 되었다면 True가, 실패했다면 False가 return 되도록 try - except로 이 코드 부분을 감싸줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "\n",
    "def download_image(url, fname):\n",
    "    try:\n",
    "        downloaded_file = open(fname, \"wb\")\n",
    "        opened = urllib.request.urlopen(url)\n",
    "        while True:\n",
    "            buffer = opened.read(100000000)\n",
    "            if len(buffer) == 0:\n",
    "                break\n",
    "            downloaded_file.write(buffer)\n",
    "        downloaded_file.close()\n",
    "        opened.close()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만든 함수로 google logo를 다운로드 받아봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_image('https://www.google.co.kr/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png', 'google_logo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "다운로드한 이미지는 아래와 같습니다. \n",
    "\n",
    "![google_logo](google_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
