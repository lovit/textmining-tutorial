{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn (sklearn) 은 다양한 머신러닝 알고리즘 및 데이터 처리 모듈들을 가지고 있는 툴킷입니다. sklearn 의 CountVectorizer 를 이용하여 term frequency matrix 를 만드는 법을 연습합니다. \n",
    "\n",
    "이를 위해서 10 개의 영화 리뷰 데이터를 모아둔 파일을 이용하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawtext_fname = '../../../tutorial_data/lalaland_comments.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일이 어떤 형식으로 저장되어 있는지 확인하기 위하여 맨 앞 5줄만 출력해봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시사회에서 보고왔습니다동화와 재즈뮤지컬의 만남 지루하지않고 재밌습니다\t9\n",
      "사랑과 꿈 그 흐름의 아름다움을 음악과 영상으로 최대한 담아놓았다 배우들 연기는 두말할것없고\t10\n",
      "지금껏 영화 평가 해본 적이 없는데 진짜 최고네요 색감 스토리 음악 연기 모두ㅜㅜ최고입니다\t10\n",
      "방금 시사회 보고 왔어요 배우들 매력이 눈을 뗄 수가 없게 만드네요 한편의 그림 같은 장면들도 많고 음악과 춤이 눈과 귀를 사로 잡았어요 한번 더 보고 싶네요\t10\n",
      "초반부터 끝까지 재미있게 잘보다가 결말에서 고국마 왕창먹음 힐링 받는 느낌들다가 막판에 기분 잡쳤습니다 마치 감독이 하고싶은 말은 너희들이 원하는 결말은 이거지 하지만 현실은 이거다 라고 말하고 싶었나보군요\t1\n"
     ]
    }
   ],
   "source": [
    "with open(rawtext_fname, encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터는 <텍스트, 평점> 이 tap 으로 구분되어 있습니다. reveiw 를 로딩하는 함수를 만들어봅니다. 테스트를 위하여 앞의 1,000 줄의 리뷰만 가져옵니다.\n",
    "\n",
    "데이터를 로딩할 수 있는 함수는 데이터 폴더의 data_loader.py 안에 넣어뒀습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../tutorial_data/')\n",
    "\n",
    "from data_loader import get_lalaland_comments\n",
    "\n",
    "texts, scores = get_lalaland_comments(n_limit=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. From corpus to noun term frequency sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter.nouns(sent) 는 주어진 sent 에서 명사를 추출하여 return 합니다. type 은 list of str 입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이건', '테스트']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "komoran.nouns('이건 테스트입니다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collections.Counter 를 이용하여 한 번에 명사의 개수를 세어 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of nouns: 1291\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "noun_counter = Counter(\n",
    "    [noun for text in texts for noun in komoran.nouns(text)]\n",
    ")\n",
    "\n",
    "print('num of nouns: %d' % len(noun_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "term frequency matrix 를 만들 때, 거의 등장하지 않는 단어들은 의미가 없습니다. min count 를 설정하여 각 threshold 별로 몇 개의 명사가 살아남는지 알아봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of nouns (min_count = 2): 572\n",
      "num of nouns (min_count = 3): 375\n",
      "num of nouns (min_count = 5): 234\n",
      "num of nouns (min_count = 10): 118\n"
     ]
    }
   ],
   "source": [
    "for min_count in [2, 3, 5, 10]:\n",
    "    _counter = {\n",
    "        word for word, freq in noun_counter.items()\n",
    "        if freq >= min_count\n",
    "    }\n",
    "    print('num of nouns (min_count = %d): %d' % (\n",
    "        min_count, len(_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 2번 이상 나오는 572 개의 명사를 이용하여 term frequency matrix 를 만들어 봅니다. 우리는 전체 문서에서의 빈도수가 2 이상인 명사만을 return 하는 토크나이저를 만들 것입니다.\n",
    "\n",
    "Komoran.nouns() 와 custom_tokenizer의 결과가 달라짐을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사랑', '꿈', '흐름', '음악', '영상', '최대한', '배우', '연기', '두말', '것']\n",
      "['사랑', '꿈', '흐름', '음악', '영상', '배우', '연기', '것']\n"
     ]
    }
   ],
   "source": [
    "noun_dict = {\n",
    "    word for word, freq in noun_counter.items()\n",
    "    if freq >= 2\n",
    "}\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    return [word for word in komoran.nouns(doc) if word in noun_dict]\n",
    "\n",
    "print(komoran.nouns(texts[1]))\n",
    "print(custom_tokenizer(texts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(noun_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer 는 document frequency 를 기준으로 대부분 문서에 등장하거나 거의 등장하지 않는 극단적인 단어들을 제거할 수 있습니다. min_df 와 max_df 를  이용하면 되며, df 는 비율입니다. [0, 1] 사이의 값을 입력해야 합니다. \n",
    "\n",
    "CountVectorizer 의 argument tokenizer 는 str 형식의 문장이 들어왔을 때 이를 list of str 형식의 단어열로 변형하는 함수입니다. 기본값은 lambda x:x.split()으로 띄어쓰기 기준으로 단어를 나눕니다. 우리가 만든 custom_tokenizer 를 이용하여 빈도수가 2 이상인 명사들만을 return 하는 토크나이저로 이를 대채합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    min_df=0.005,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "x_sparse = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_sparse 는 scipy 의 sparse matrix 입니다. Sparse matrix 는 0 값이 많은 행렬에 대하여 0 이 아닌 (i, j) 의 값만을 저장한 matrix 를 의미합니다. Term frequency matrix 는 very sparse matrix 에 속하므로 dense matrix 보다 sparse matrix 가 훨씬 효율적입니다. dense matrix 는 문서의 양이 조금만 커도 매우 큰 메모리가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_sparse 는 1,000 개의 리뷰가 223 개의 단어로 표현된 형태입니다. 이는 우리가 min_df = 0.005 로 설정하였기 때문입니다. Document frequency 가 4 이하인 단어들도 제거되었기 때문입니다.\n",
    "\n",
    "또한 이 sparse matrix 는 CSR matrix 형식입니다. sparse matrix 의 형식은 이후에 다시 설명하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1000x223 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4499 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(x_sparse))\n",
    "x_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CountVectorizer 에서 각 column 에 해당하는 term 알아내기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorizer.vocabulary_ 는 dict 형식으로, {word:index} 입니다. \n",
    "\n",
    "    {'가장': 0,\n",
    "     '가족': 1,\n",
    "     '가치': 2,\n",
    "     '각본': 3,\n",
    "     '간다': 4,\n",
    "     '감': 5,\n",
    "     '감독': 6,\n",
    "     ...\n",
    "    }\n",
    "\n",
    "이를 이용하여 vocab to index, index to vocab 을 만들어 봅니다. 영화라는 단어가 115 에 해당합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2int = vectorizer.vocabulary_\n",
    "vocab2int['영화']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index 는 0 부터 시작합니다. 그러므로 int2vocab 은 dictionary 형태보다 list 형태로 가지고 있어도 좋습니다. 이를 위해 sorted 함수를 이용합니다. 115 번째 단어가 '영화'임을 다시 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'영화'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2vocab = [\n",
    "    word for word,index in sorted(\n",
    "        vocab2int.items(), key=lambda x:x[1])\n",
    "]\n",
    "int2vocab[134]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sparse matrix I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy.io 는 sparse matrix 를 읽고 쓰는 기능을 제공합니다. mmwrite 는 sparse matrix 를 filepath 에 저장합니다. 그런데 filepath 의 경로에 directory 가 존재하지 않는다면 오류가 발생합니다. 이를 방지하기 위해서 path 가 존재하는지 확인하고, directory 가 존재하지 않을 경우 이를 만들어야 합니다.\n",
    "\n",
    "    if not os.path.exists('tmp/'):\n",
    "        os.makedirs('tmp')\n",
    "\n",
    "위의 코드는 tmp 폴더가 있는지 확인하고, 존재하지 않는 경우 tmp 폴더를 만드는 코드입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import mmwrite, mmread\n",
    "\n",
    "if not os.path.exists('tmp/'):\n",
    "    os.makedirs('tmp')\n",
    "\n",
    "mmwrite('./tmp/x.mm', x_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mmread 는 filepath 의 sparse matrix 를 읽습니다. 그런데 mmread 의 형식은 COO matrix 입니다. 이를 CSR matrix 로 바꾸기 위해 tocsr 함수를 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x223 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4499 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_x_sparse = mmread('./tmp/x.mm').tocsr()\n",
    "loaded_x_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save vectorizer and vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorizer 는 pickling 으로 저장하여도 좋습니다. vocabulary list 는 따로 list 파일로 저장해도 좋습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./tmp/vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "with open('./tmp/vocabulary.txt', 'w', encoding='utf-8') as f:\n",
    "    for vocab in int2vocab:\n",
    "        f.write('%s\\n' % vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "with open('./tmp/vectorizer.pkl', 'rb') as f:\n",
    "    loaded_vectorizer = pickle.load(f)\n",
    "\n",
    "print(loaded_vectorizer.vocabulary_['영화'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10분\n",
      "가슴\n",
      "감\n",
      "감독\n",
      "감동\n"
     ]
    }
   ],
   "source": [
    "with open('./tmp/vocabulary.txt', encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
